#import required libraries
import twitter
import pandas as pd
import string
from nltk.corpus import stopwords
# load previously created naive bayes model 
from sklearn.externals import joblib
nb_model = joblib.load('NB_Model_twitter.pkl')

# Get tokens & secrets from twitter dev account
CONSUMER_KEY = 'CONSUMER_KEY'
CONSUMER_SECRET = 'CONSUMER_SECRET'
ACCESS_TOKEN_KEY = 'ACCESS_TOKEN_KEY'
ACCESS_TOKEN_SECRET = 'ACCESS_TOKEN_SECRET'

#Create empty dataframes to be populated with trends and tweets of those trends
df_trend_hist = pd.DataFrame(columns=["name","query","timestamp","tweet_volume","url"])
df_trend_current = pd.DataFrame(columns=["name","query","timestamp","tweet_volume","url"])
df_trend_search = pd.DataFrame(columns = ["Hashtag","ID","Created","Text","Clean Text","Place","Coordinates","Favourite Count""Retweet Count","Sentiment"])

#instantiate an api instance
api = twitter.Api(consumer_key = CONSUMER_KEY,
                  consumer_secret = CONSUMER_SECRET,
                  access_token_key= ACCESS_TOKEN_KEY,
                  access_token_secret=ACCESS_TOKEN_SECRET,
                 sleep_on_rate_limit=True)

def get_trends(Woeid):
    """Uses trends/place endpoint to retrieve current trending Tweets in specified Woeid (where on earth id)
    https://developer.twitter.com/en/docs/trends/trends-for-location/api-reference/get-trends-place
    Populates data frame for current trends and concatenates current trends to historical trends data frame"""
    global df_trend_hist
    global df_trend_current
    #call twitter for trending in geographical location
    trend_response = api.GetTrendsWoeid(Woeid)
    a=[]
    #loop through trend response and update attribute values in a dictionary, append dictionaries to list
    for x in trend_response:
        dictionary = {"name":'',"query":'',"timestamp":'',"tweet_volume":0,"url":''}
        dictionary["name"]=x.name
        dictionary["query"]=x.query
        dictionary["timestamp"]=x.timestamp
        dictionary["tweet_volume"]=x.tweet_volume
        dictionary["url"]=x.url
        a.append(dictionary)
    #create data frame from list of dictionaries
    df_a = pd.DataFrame(a)
    #concatenate historical trend data frame with existing trend dataframe
    df_trend_hist = pd.concat([df_trend_hist,df_a],axis=0,sort=False)
    df_trend_hist.to_csv('Trends.csv')
    #over write current trend data frame with new trends
    df_trend_current = df_a

def text_process(message):
    #1 Split message into list of words & remove Stop Words, hashtags, @mentions & http links
    no_punc = [word for word in message.split() if word.lower() not in stopwords.words('english') 
               and word[0] != '#' 
               and word[0] != '@'
               and word[0:4] != 'http']
    #2 rejoin split word list in space separated string
    no_punc = ' '.join(no_punc)
    #3 Parse string for punctuation marks
    no_punc = [char for char in no_punc if char not in string.punctuation]
    #4 Rejoin characters into text string
    no_punc = ''.join(no_punc)
    return no_punc
    
def search_trends():
    """Uses search/tweets endpoint to retrieve Tweets relating to trending tweets
    https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets
    Utilises NB model to classify sentiment of tweet
    concatenates returned tweets with historical tweets data frame
    """
    global df_trend_search
    a = []
    #loop through trend data frame query and name columns
    for query,hashtag in zip(df_trend_current['query'],df_trend_current['name']):
        #check trend search data frame, if entries exist include since_id parameter to prevent duplicates
        if df_trend_search.empty:
            query_string = 'q='+query+'&count=100&lang=en'
        else:
            since_id = df_trend_search['ID'].max()
            query_string = 'q='+query+'&count=100&lang=en&since_id'+str(since_id)
        #perform API search call on twitter
        response = api.GetSearch(raw_query=query_string)
        #loop through response extracting attributes saving to dictionary, append dictionaries to list
        for x in response:
            dictionary = {"Hashtag":'',"ID":'',"Created":'',"Text":'',"Place":'',"Coordinates":'',"Favourite Count":0,"Retweet Count":0}
            dictionary["Hashtag"] = hashtag
            dictionary["ID"] = x.id
            dictionary["Created"] = x.created_at
            dictionary["Text"] = x.text
            dictionary["Place"] = x.place
            dictionary["Coordinates"] = x.coordinates
            dictionary["Favourite Count"] = x.favorite_count
            dictionary["Retweet Count"] = x.retweet_count
            a.append(dictionary)
    #create dataframe from dictionaries list
    df_a = pd.DataFrame(a)
    df_a.head()
    #clean df_a text column using text process function, create clean text column
    df_a['Clean Text'] = df_a['Text'].apply(text_process)
    #create sentiment identifier column in df_a
    df_a['Sentiment'] = nb_model.predict(df_a['Clean Text'])
    #concatenate df_a data frame to df_trend_search
    df_trend_search = pd.concat([df_trend_search,df_a],axis=0,sort=False)
    df_trend_search.to_csv('Tweets.csv')

# use boto to save files to AWS S3
import boto3
s3 = boto3.client('s3')

def send_to_s3(filename):
    bucket_name = 'ciaran-test-twitter-bucket'
    s3.upload_file(filename, bucket_name, filename)
    
# import schedule library to facilitate process automation
# SafeScheduler code available https://gist.github.com/mplewis/8483f1c24f2d6259aef6
from safe_schedule import SafeScheduler
import time
from datetime import datetime

initial_flag = False

def initial_job():
    count = 1
    global initial_flag
    if initial_flag == False:
        print('Running initial job, calls Trends and then searches Trends for tweets')
        get_trends(4118)
        search_trends()
        print(count)
        count +=1 
        initial_flag = True
        print('Initial Job completed, Initial flag:'+str(initial_flag))
        return initial_flag
    #return scheduler.CancelJob
    
def trend_job():
    #global initial_flag
    #if initial_flag == True:
        print('Twitter Trend GET Call')
        print(str(datetime.now()))
        get_trends(4118)
    #else:
        #print('Initial flag is'+str(initial_flag)+' trend_job() will not run')

def trend_search_job():
    #global initial_flag
    #if initial_flag == True:
        print('Twitter Search Trend GET Call')
        print(str(datetime.now()))
        search_trends()
    #else:
        #print('Initial flag is'+str(initial_flag)+' trend_search_job() will not run')

scheduler = SafeScheduler()
scheduler.every(1).seconds.do(initial_job)
scheduler.every(15).minutes.do(trend_job)
scheduler.every(1).minutes.do(trend_search_job)
scheduler.every(15).minutes.do(send_to_s3, 'Tweets.csv')
scheduler.every(15).minutes.do(send_to_s3, 'Trends.csv')

while 1:
    scheduler.run_pending()
    time.sleep(1)